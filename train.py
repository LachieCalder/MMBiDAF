"""
Train a model on the MMS Dataset.
"""
import os
import torch
import random
import logging
import pickle
import torchvision
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torch.nn.functional as F
import torch.optim.lr_scheduler as sched
import torchvision.transforms as transforms

from collections import OrderedDict
from PIL import Image
from json import dumps
# from models import MMBiDAF
from tensorboardX import SummaryWriter
from tqdm import tqdm
from ujson import load as json_load
from datasets import ImageDataset

def main(sent_embedding_path, audio_path, image_dir):
    # Get sentence embeddings
    """
    The sentence embeddings need to be imported from the
    sentence embeddings generated by using gensim.
    """
    sent_embedding_dict = torch.load(sent_embedding_path)                  #TODO : The absolute path needs to be changed 
    
    print('Loading sentence embeddings...')
    
    word_vectors = torch.zeros(len(sent_embedding_dict),300)
    for count, sentence in enumerate(sent_embedding_dict):
        word_vectors[count] = sent_embedding_dict[sentence]
    
    print(word_vectors) 
    print('The shape is : {}'.format(word_vectors.size()))

    # Get Audio embeddings
    """
    The features are imported from the extracted MFCC features. 
    """
    with open(audio_path, 'rb') as fp:
        audio_vectors = pickle.load(fp)
    
    audio_vectors = np.transpose(audio_vectors)
    audio_vectors = torch.from_numpy(audio_vectors)

    print(audio_vectors)
    print('The shape is : {}'.format(audio_vectors.size()))
    
    # Preprocess the image in prescribed format
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])
    
    train_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = True, num_workers = 2)
    
    # Get model
    logging.info('Building model')
    model = MMBiDAF(word_vector = word_vectors,
                    audio_vectors = audio_vectors,
                    image_vectors = image_vectors,
                    hidden_size = args.hidden_size,
                    drop_prob = args.drop_prob)
    
if __name__ == '__main__':
    embedding_path = '/home/anish17281/NLP_Dataset/dataset/1/sentence_features/1.pt'
    audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/1.pkl'
    image_dir = '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/'
    main(embedding_path, audio_path, image_dir)
