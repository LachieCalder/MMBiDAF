{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on the MMS Dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from json import dumps\n",
    "# from models import MMBiDAF\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from multimodal_bidaf.datasets import ImageDataset\n",
    "\n",
    "def main(embedding_path, audio_path, image_dir):\n",
    "    # Get embeddings\n",
    "    \"\"\"\n",
    "    The embeddings need to be imported from the\n",
    "    sentence embeddings generated by using gensim.\n",
    "    \"\"\"\n",
    "    embedding_dict = torch.load(embedding_path)                  #TODO : The absolute path needs to be changed \n",
    "    \n",
    "    print('Loading embeddings...')\n",
    "    \n",
    "    word_vectors = torch.zeros(len(embedding_dict),300)\n",
    "    for count, embedding in enumerate(embedding_dict):\n",
    "        word_vectors[count] = embedding_dict[embedding]\n",
    "    \n",
    "    print(word_vectors) \n",
    "    print('The shape is : {}'.format(word_vectors.size()))\n",
    "\n",
    "    # Get Audio embeddings\n",
    "    \"\"\"\n",
    "    The features are imported from the extracted MFCC features. \n",
    "    \"\"\"\n",
    "    with open(audio_path, 'rb') as fp:\n",
    "        audio_vectors = pickle.load(fp)\n",
    "    \n",
    "    audio_vectors = np.transpose(audio_vectors)\n",
    "    audio_vectors = torch.from_numpy(audio_vectors)\n",
    "\n",
    "    print(audio_vectors)\n",
    "    print('The shape is : {}'.format(audio_vectors.size()))\n",
    "    \n",
    "    # Preprocess the image in prescribed format\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = True, num_workers = 2)\n",
    "    \n",
    "    # Get model\n",
    "    log.info('Building model')\n",
    "    model = MMBiDAF(word_vector = word_vectors,\n",
    "                    audio_vectors = audio_vectors,\n",
    "                    image_vectors = image_vectors,\n",
    "                    hidden_size = args.hidden_size,\n",
    "                    drop_prob = args.drop_prob)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    embedding_path = '/home/anish17281/NLP_Dataset/dataset/1/sentence_features/1.pt'\n",
    "    audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/1.pkl'\n",
    "    image_dir = '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/'\n",
    "    main(embedding_path, audio_path, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Image Encoder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ImageDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])\n",
    "image_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "import os\n",
    "from layers import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset(image_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_1.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_2.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_3.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_4.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_5.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_6.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_7.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_8.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_9.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_10.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_11.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_12.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_13.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_14.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_15.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_16.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_17.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_18.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_19.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_20.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_21.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_22.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_23.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_24.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_25.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_26.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_27.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_28.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_29.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_30.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_31.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_32.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_33.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_34.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_35.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_36.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_37.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_38.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_39.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_40.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_41.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_42.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_43.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_44.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_45.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_46.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_47.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_48.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_49.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_50.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_51.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_52.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_53.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_54.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_55.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_56.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_57.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_58.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_59.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_60.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_61.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_62.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_63.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_64.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_65.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_66.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_67.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_68.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_69.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_70.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_71.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_72.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_73.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_74.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_75.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_76.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_77.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_78.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_79.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_80.jpg',\n",
       " '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/1_i_frame_81.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset.image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = False, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f18a1eda390>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = ImageEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 401408])\n",
      "torch.Size([81, 300])\n"
     ]
    }
   ],
   "source": [
    "batch_encodings = torch.Tensor(0,0)\n",
    "for i,batch_images in enumerate(train_loader):\n",
    "    original_shape = batch_images.size()\n",
    "    batch_images = torch.reshape(batch_images, (-1, batch_images.size(2), batch_images.size(3), batch_images.size(4)))\n",
    "    batch_encodings = image_encoder(batch_images)\n",
    "    batch_encodings = torch.reshape(batch_encodings, (batch_encodings.size(0), -1))\n",
    "    print(batch_encodings.size())\n",
    "    break\n",
    "    \n",
    "m_linear = nn.Linear(batch_encodings.size(-1), 300)\n",
    "output = m_linear(batch_encodings)\n",
    "print(output.size())\n",
    "# for count, batch_images in enumerate(train_loader):\n",
    "#     print(type(batch_images))\n",
    "#     print(batch_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset(image_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_dataset.images))\n",
    "print(image_dataset.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sorted_image_dir = sorted(os.listdir(image_dir), key = int)\n",
    "images = []\n",
    "\n",
    "def get_num(str):\n",
    "    return int(re.search(r'\\d+', re.search(r'_\\d+', str).group()).group())\n",
    "\n",
    "for video_path in sorted_image_dir:\n",
    "    keyframes = [os.path.join(video_path, img) for img in os.listdir(os.path.join(image_dir, video_path)) \\\n",
    "                if os.path.isfile(os.path.join(image_dir, video_path, img))]\n",
    "    keyframes.sort(key = get_num)\n",
    "    images.extend([keyframes])\n",
    "    \n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0][0])\n",
    "# print(os.path.join(image_dir, images[0][0]))\n",
    "print(type(transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing TextEmbedding in layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import TextEmbedding\n",
    "from multimodal_bidaf.layers import HighwayEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.datasets import TextDataset, ImageDataset, AudioDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "text_embedding_dir = '/home/anish17281/NLP_Dataset/dataset/1/sentence_features/'\n",
    "train_text_loader = torch.utils.data.DataLoader(TextDataset(text_embedding_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, batch_text in enumerate(train_text_loader):\n",
    "    print(batch_text)\n",
    "    print(batch_text.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "embedding_dict = torch.load('/home/anish17281/NLP_Dataset/dataset/1/sentence_features/1.pt')\n",
    "for count, embedding in enumerate(embedding_dict):\n",
    "    word_vectors[count] = embedding_dict[embedding]\n",
    "    \n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = TextEmbedding(word_vectors=word_vectors, hidden_size=100, drop_prob=0.2)\n",
    "print(type(emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "embed = nn.Embedding.from_pretrained(word_vectors)\n",
    "input = torch.LongTensor([[0, 1], [2,3]])\n",
    "print(embed(input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import RNNEncoder\n",
    "\n",
    "enc = RNNEncoder(input_size=100, hidden_size=100, num_layers=1, drop_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb = emb(input)\n",
    "print(text_emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = enc(text_emb, torch.Tensor([2, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.LongTensor([[2, 1], [3, 1]])\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bool = torch.zeros_like(temp) != temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_bool.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sum = temp_bool.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_enc.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing Audio Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/7.pkl'\n",
    "with open(audio_dir, 'rb') as fp:\n",
    "    audio_vectors = pickle.load(fp)\n",
    "    \n",
    "print(audio_vectors)\n",
    "print(audio_vectors.shape)\n",
    "audio_vectors = np.transpose(audio_vectors)\n",
    "audio_vectors = torch.from_numpy(audio_vectors)\n",
    "print(audio_vectors.size())\n",
    "print(type(audio_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_num(str):\n",
    "    return int(re.search(r'\\d+',str).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/'\n",
    "audio_list = sorted(os.listdir(audio_path), key = get_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(audio_path, audio_list[0]), 'rb') as fp:\n",
    "    a_v = pickle.load(fp)\n",
    "print(a_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.datasets import AudioDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "audio_dir = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/'\n",
    "train_loader_1 = torch.utils.data.DataLoader(AudioDataset(audio_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(audio_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, batch_audio in enumerate(train_loader_1):\n",
    "    print(batch_audio)\n",
    "    print(batch_audio.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_audio.size(1))\n",
    "temp_audio = torch.randn(1, 10, 20)\n",
    "print(temp_audio.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import RNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder = RNNEncoder(input_size=20, hidden_size=300, num_layers=3, drop_prob=0.2)\n",
    "x, _ = audio_encoder.rnn(temp_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Bidirectional Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from multimodal_bidaf.layers import TextImageBiDAFAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.randn(1, 81, 200)\n",
    "text_tensor = torch.randn(1, 48, 200)\n",
    "audio_tensor = torch.randn(1, 32453, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_image_att = TextImageBiDAFAttention(hidden_size=200, drop_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_idxs = torch.randn(1, 48)\n",
    "img_idxs = torch.randn(1, 81)\n",
    "\n",
    "c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "img_mask = torch.zeros_like(img_idxs) != img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_image_attention = text_image_att(text_tensor, image_tensor, c_mask, img_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.util import masked_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
