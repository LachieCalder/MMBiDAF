{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on the MMS Dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from json import dumps\n",
    "# from models import MMBiDAF\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from multimodal_bidaf.datasets import ImageDataset\n",
    "\n",
    "def main(embedding_path, audio_path, image_dir):\n",
    "    # Get embeddings\n",
    "    \"\"\"\n",
    "    The embeddings need to be imported from the\n",
    "    sentence embeddings generated by using gensim.\n",
    "    \"\"\"\n",
    "    embedding_dict = torch.load(embedding_path)                  #TODO : The absolute path needs to be changed \n",
    "    \n",
    "    print('Loading embeddings...')\n",
    "    \n",
    "    word_vectors = torch.zeros(len(embedding_dict),300)\n",
    "    for count, embedding in enumerate(embedding_dict):\n",
    "        word_vectors[count] = embedding_dict[embedding]\n",
    "    \n",
    "    print(word_vectors) \n",
    "    print('The shape is : {}'.format(word_vectors.size()))\n",
    "\n",
    "    # Get Audio embeddings\n",
    "    \"\"\"\n",
    "    The features are imported from the extracted MFCC features. \n",
    "    \"\"\"\n",
    "    with open(audio_path, 'rb') as fp:\n",
    "        audio_vectors = pickle.load(fp)\n",
    "    \n",
    "    audio_vectors = np.transpose(audio_vectors)\n",
    "    audio_vectors = torch.from_numpy(audio_vectors)\n",
    "\n",
    "    print(audio_vectors)\n",
    "    print('The shape is : {}'.format(audio_vectors.size()))\n",
    "    \n",
    "    # Preprocess the image in prescribed format\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = True, num_workers = 2)\n",
    "    \n",
    "    # Get model\n",
    "    log.info('Building model')\n",
    "    model = MMBiDAF(word_vector = word_vectors,\n",
    "                    audio_vectors = audio_vectors,\n",
    "                    image_vectors = image_vectors,\n",
    "                    hidden_size = args.hidden_size,\n",
    "                    drop_prob = args.drop_prob)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    embedding_path = '/home/anish17281/NLP_Dataset/dataset/1/sentence_features/1.pt'\n",
    "    audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/1.pkl'\n",
    "    image_dir = '/home/anish17281/NLP_Dataset/dataset/1/video_key_frames/1/'\n",
    "    main(embedding_path, audio_path, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Image Encoder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ImageDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])\n",
    "image_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "import os\n",
    "# from layers import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 1000])\n"
     ]
    }
   ],
   "source": [
    "from layers.encoding import ImageEmbedding\n",
    "\n",
    "image_keyframes_emb = ImageEmbedding(encoded_image_size=2)\n",
    "\n",
    "train_image_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = False, num_workers = 2)\n",
    "\n",
    "for transformed_images in train_image_loader:\n",
    "    original_images_size = transformed_images.size()                                             # (batch_size, num_keyframes, num_channels, transformed_image_size, transformed_image_size)\n",
    "    # Combine images across videos in a batch into a single dimension to be embedded by ResNet\n",
    "    transformed_images = torch.reshape(transformed_images, (-1, transformed_images.size(2), transformed_images.size(3), transformed_images.size(4)))    # (batch_size * num_keyframes, num_channels, transformed_image_size, transformed_image_size)\n",
    "    image_emb = image_keyframes_emb(transformed_images)                                    # (batch_size * num_keyframes, encoded_image_size)\n",
    "    print(image_emb.size())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset(image_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-268becf09d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "image_dataset.image_paths[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = False, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f18a1eda390>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = ImageEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 401408])\n",
      "torch.Size([81, 300])\n"
     ]
    }
   ],
   "source": [
    "batch_encodings = torch.Tensor(0,0)\n",
    "for i,batch_images in enumerate(train_loader):\n",
    "    original_shape = batch_images.size()\n",
    "    batch_images = torch.reshape(batch_images, (-1, batch_images.size(2), batch_images.size(3), batch_images.size(4)))\n",
    "    batch_encodings = image_encoder(batch_images)\n",
    "    batch_encodings = torch.reshape(batch_encodings, (batch_encodings.size(0), -1))\n",
    "    print(batch_encodings.size())\n",
    "    break\n",
    "    \n",
    "m_linear = nn.Linear(batch_encodings.size(-1), 300)\n",
    "output = m_linear(batch_encodings)\n",
    "print(output.size())\n",
    "# for count, batch_images in enumerate(train_loader):\n",
    "#     print(type(batch_images))\n",
    "#     print(batch_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset(image_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_dataset.images))\n",
    "print(image_dataset.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sorted_image_dir = sorted(os.listdir(image_dir), key = int)\n",
    "images = []\n",
    "\n",
    "def get_num(str):\n",
    "    return int(re.search(r'\\d+', re.search(r'_\\d+', str).group()).group())\n",
    "\n",
    "for video_path in sorted_image_dir:\n",
    "    keyframes = [os.path.join(video_path, img) for img in os.listdir(os.path.join(image_dir, video_path)) \\\n",
    "                if os.path.isfile(os.path.join(image_dir, video_path, img))]\n",
    "    keyframes.sort(key = get_num)\n",
    "    images.extend([keyframes])\n",
    "    \n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0][0])\n",
    "# print(os.path.join(image_dir, images[0][0]))\n",
    "print(type(transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing TextEmbedding in layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import TextEmbedding\n",
    "from multimodal_bidaf.layers import HighwayEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.datasets import TextDataset, ImageDataset, AudioDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "text_embedding_dir = '/home/anish17281/NLP_Dataset/dataset/1/sentence_features/'\n",
    "train_text_loader = torch.utils.data.DataLoader(TextDataset(text_embedding_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, batch_text in enumerate(train_text_loader):\n",
    "    print(batch_text)\n",
    "    print(batch_text.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "embedding_dict = torch.load('/home/anish17281/NLP_Dataset/dataset/1/sentence_features/1.pt')\n",
    "for count, embedding in enumerate(embedding_dict):\n",
    "    word_vectors[count] = embedding_dict[embedding]\n",
    "    \n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = TextEmbedding(word_vectors=word_vectors, hidden_size=100, drop_prob=0.2)\n",
    "print(type(emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "embed = nn.Embedding.from_pretrained(word_vectors)\n",
    "input = torch.LongTensor([[0, 1], [2,3]])\n",
    "print(embed(input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import RNNEncoder\n",
    "\n",
    "enc = RNNEncoder(input_size=100, hidden_size=100, num_layers=1, drop_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb = emb(input)\n",
    "print(text_emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = enc(text_emb, torch.Tensor([2, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.LongTensor([[2, 1], [3, 1]])\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bool = torch.zeros_like(temp) != temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_bool.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sum = temp_bool.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_enc.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing Audio Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/7.pkl'\n",
    "with open(audio_dir, 'rb') as fp:\n",
    "    audio_vectors = pickle.load(fp)\n",
    "    \n",
    "print(audio_vectors)\n",
    "print(audio_vectors.shape)\n",
    "audio_vectors = np.transpose(audio_vectors)\n",
    "audio_vectors = torch.from_numpy(audio_vectors)\n",
    "print(audio_vectors.size())\n",
    "print(type(audio_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_num(str):\n",
    "    return int(re.search(r'\\d+',str).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "audio_path = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/'\n",
    "audio_list = sorted(os.listdir(audio_path), key = get_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(audio_path, audio_list[0]), 'rb') as fp:\n",
    "    a_v = pickle.load(fp)\n",
    "print(a_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.datasets import AudioDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "audio_dir = '/home/anish17281/NLP_Dataset/dataset/1/audio-features/'\n",
    "train_loader_1 = torch.utils.data.DataLoader(AudioDataset(audio_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(audio_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, batch_audio in enumerate(train_loader_1):\n",
    "    print(batch_audio)\n",
    "    print(batch_audio.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_audio.size(1))\n",
    "temp_audio = torch.randn(1, 10, 20)\n",
    "print(temp_audio.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.layers import RNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder = RNNEncoder(input_size=20, hidden_size=300, num_layers=3, drop_prob=0.2)\n",
    "x, _ = audio_encoder.rnn(temp_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Bidirectional Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from multimodal_bidaf.layers import TextImageBiDAFAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.randn(1, 81, 200)\n",
    "text_tensor = torch.randn(1, 48, 200)\n",
    "audio_tensor = torch.randn(1, 32453, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_image_att = TextImageBiDAFAttention(hidden_size=200, drop_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_idxs = torch.randn(1, 48)\n",
    "img_idxs = torch.randn(1, 81)\n",
    "\n",
    "c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "img_mask = torch.zeros_like(img_idxs) != img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_image_attention = text_image_att(text_tensor, image_tensor, c_mask, img_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_bidaf.util import masked_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
