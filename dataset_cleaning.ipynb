{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which transcripts are missing, but have images/audios - to generate a dataset where each video has the images, audio and text features along with targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "courses_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "\n",
    "# Get sorted list of all courses (excluding any files)\n",
    "dirlist = []\n",
    "for fname in os.listdir(courses_dir):\n",
    "    if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "        dirlist.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing course 1\n",
      "Processing course 2\n",
      "Processing course 3\n",
      "Processing course 4\n",
      "Processing course 5\n",
      "Processing course 6\n",
      "Processing course 7\n",
      "Processing course 8\n",
      "Processing course 9\n",
      "Processing course 10\n",
      "Processing course 11\n",
      "Processing course 12\n",
      "Processing course 13\n",
      "Processing course 14\n",
      "Processing course 15\n",
      "Processing course 16\n",
      "Processing course 17\n",
      "Processing course 18\n",
      "Processing course 19\n",
      "Processing course 20\n",
      "Processing course 21\n",
      "Processing course 22\n",
      "Processing course 23\n",
      "Processing course 24\n"
     ]
    }
   ],
   "source": [
    "vids = set()\n",
    "trs = set()\n",
    "gts = set()\n",
    "audios = set()\n",
    "\n",
    "for course_num in sorted(dirlist, key=int):\n",
    "    print(\"Processing course \" + str(course_num))\n",
    "    for vid in os.listdir(os.path.join(courses_dir, course_num, 'videos')):\n",
    "        if 'mp4' not in vid or '_' in vid:\n",
    "            continue\n",
    "        vids.add('{}/{}'.format(course_num, vid[:-4]))\n",
    "    \n",
    "    for audio in os.listdir(os.path.join(courses_dir, course_num, 'audio-features')):\n",
    "        if 'pkl' not in audio or '_' in audio:\n",
    "            continue\n",
    "        audios.add('{}/{}'.format(course_num, audio[:-4]))\n",
    "    \n",
    "    for tr in os.listdir(os.path.join(courses_dir, course_num, 'transcripts')):\n",
    "        if 'txt' not in tr or '_' in tr:\n",
    "            continue\n",
    "        trs.add('{}/{}'.format(course_num, tr[:-4]))\n",
    "    \n",
    "    for gt in os.listdir(os.path.join(courses_dir, course_num, 'ground-truth')):\n",
    "        if 'txt' not in gt or '_' in gt:\n",
    "            continue\n",
    "        gts.add('{}/{}'.format(course_num, gt[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 965, 961, 961)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vids), len(audios), len(trs), len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter = vids.intersection(audios).intersection(trs).intersection(gts)\n",
    "len(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "with open('dataset_inter.pkl', 'wb') as f:\n",
    "    pickle.dump(inter, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(str):\n",
    "    return int(re.search(r'\\d+', str).group())\n",
    "\n",
    "def load_sentence_embeddings_path():\n",
    "    transcript_embeddings = []\n",
    "\n",
    "    # Get sorted list of all courses (excluding any files)\n",
    "    dirlist = []\n",
    "    for fname in os.listdir(courses_dir):\n",
    "        if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "            dirlist.append(fname)\n",
    "\n",
    "    for course_number in sorted(dirlist, key=int):\n",
    "        course_transcript_path = os.path.join(courses_dir, course_number, 'sentence_features/')\n",
    "        text_embedding_path = [courses_dir + course_number + '/sentence_features/' + transcript_path for transcript_path in sorted(os.listdir(course_transcript_path), key=get_num)]\n",
    "        transcript_embeddings.append(text_embedding_path)\n",
    "    \n",
    "    return transcript_embeddings\n",
    "\n",
    "def load_image_paths():\n",
    "    images = []\n",
    "\n",
    "    # Get sorted list of all courses (excluding any files)\n",
    "    dirlist = []\n",
    "    for fname in os.listdir(courses_dir):\n",
    "        if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "            dirlist.append(fname)\n",
    "\n",
    "    for course_dir in sorted(dirlist, key=int):\n",
    "        keyframes_dir_path = os.path.join(courses_dir, course_dir, 'video_key_frames/')\n",
    "        for video_dir in sorted(os.listdir(keyframes_dir_path), key=int):\n",
    "            video_dir_path = os.path.join(keyframes_dir_path, video_dir)\n",
    "            keyframes = [os.path.join(video_dir_path, img) for img in os.listdir(video_dir_path) \\\n",
    "                        if os.path.isfile(os.path.join(video_dir_path, img))]\n",
    "            keyframes.sort(key = get_num)\n",
    "            images.extend([keyframes])\n",
    "\n",
    "    return images\n",
    "\n",
    "def load_audio_path():\n",
    "    audio_embeddings = []\n",
    "\n",
    "    # Get sorted list of all courses (excluding any files)\n",
    "    dirlist = []\n",
    "    for fname in os.listdir(courses_dir):\n",
    "        if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "            dirlist.append(fname)\n",
    "\n",
    "    for course_number in sorted(dirlist, key=int):\n",
    "        course_audio_path = os.path.join(courses_dir, course_number, 'audio-features/')\n",
    "        audio_embedding_path = [courses_dir + course_number + '/audio-features/' + audio_path for audio_path in sorted(os.listdir(course_audio_path), key=get_num)]\n",
    "        audio_embeddings.append(audio_embedding_path)\n",
    "\n",
    "    return [val for sublist in audio_embeddings for val in sublist]     #Flatten the list of lists\n",
    "\n",
    "def load_target_sentences_path():\n",
    "    target_sentences = []\n",
    "    dirlist = []\n",
    "    for fname in os.listdir(courses_dir):\n",
    "        if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "            dirlist.append(fname)\n",
    "\n",
    "    for course_number in sorted(dirlist, key=int):\n",
    "        target_path = os.path.join(courses_dir, course_number, 'ground-truth/')\n",
    "        target_sentence_path = [target_path + target_sentence for target_sentence in sorted([item for item in os.listdir(target_path) if os.path.isfile(os.path.join(target_path, item)) and '.txt' in item and '_' not in item], key=get_num)]\n",
    "        target_sentences.append(target_sentence_path)\n",
    "\n",
    "    return [val for sublist in target_sentences for val in sublist]    #Flatten the list of lists\n",
    "\n",
    "def load_source_sentences_path():\n",
    "    source_sentences = []\n",
    "\n",
    "    # Get sorted list of all courses (excluding any files)\n",
    "    dirlist = []\n",
    "    for fname in os.listdir(courses_dir):\n",
    "        if os.path.isdir(os.path.join(courses_dir, fname)):\n",
    "            dirlist.append(fname)\n",
    "\n",
    "    for course_number in sorted(dirlist, key=int):\n",
    "        source_path = os.path.join(courses_dir, course_number, 'transcripts/')\n",
    "        source_sentence_path = [source_path + transcript_path for transcript_path in sorted([item for item in os.listdir(source_path) if os.path.isfile(os.path.join(source_path, item)) and '.txt' in item], key=get_num)]\n",
    "\n",
    "        source_sentences.append(source_sentence_path)\n",
    "\n",
    "    return [val for sublist in source_sentences for val in sublist]    #Flatten the list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import TextDataset, ImageDataset, AudioDataset, TargetDataset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "train_text_loader = torch.utils.data.DataLoader(TextDataset(text_embedding_dir, 405), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(256), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize,])\n",
    "\n",
    "image_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "train_image_loader = torch.utils.data.DataLoader(ImageDataset(image_dir, transform), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "train_audio_loader = torch.utils.data.DataLoader(AudioDataset(audio_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_dir = '/home/anish17281/NLP_Dataset/dataset/'\n",
    "train_target_loader = torch.utils.data.DataLoader(TargetDataset(courses_dir), batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_audio_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_target_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(961, 961)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_target_loader.dataset.target_sentences_path), len(train_target_loader.dataset.source_sentences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
